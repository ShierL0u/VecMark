nohup: ignoring input
/root/autodl-tmp/vectormark/utils1.py:17: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.

For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`
with: `from pydantic import BaseModel`
or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. 	from pydantic.v1 import BaseModel

  from KGW.KGW_claude import KGWWatermarkedLLM
wm_bits:  [1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1]

Saved processed data to: /root/autodl-tmp/vectormark/data/data_FCT.csv
================================================
Is:  [('Aspect', 'Slope', '', ''), ('Aspect', '', 'Hillshade_9am', ''), ('Aspect', '', 'Hillshade_9am', 'Hillshade_Noon'), ('', 'Slope', '', 'Hillshade_Noon'), 'id']
Loading model: ../model/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8

[32mINFO[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          
[32mINFO[0m   Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`          
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|â–ˆ         | 1/9 [00:00<00:05,  1.36it/s]Loading checkpoint shards:  22%|â–ˆâ–ˆâ–       | 2/9 [00:01<00:05,  1.32it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:02<00:04,  1.31it/s]Loading checkpoint shards:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:03<00:03,  1.30it/s]Loading checkpoint shards:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:03<00:03,  1.29it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:04<00:02,  1.28it/s]Loading checkpoint shards:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:05<00:01,  1.24it/s]Loading checkpoint shards:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:06<00:00,  1.25it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:06<00:00,  1.35it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:06<00:00,  1.31it/s]
[32mINFO[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.
[32mINFO[0m  Format: Converting GPTQ v1 to v2                                         
[32mINFO[0m  Format: Conversion complete: 0.015554666519165039s                       
[32mINFO[0m  Optimize: `TritonV2QuantLinear` compilation triggered.                   
Model loaded successfully
Embedding watermark:   0%|          | 0/581012 [00:00<?, ?it/s]Embedding watermark:   0%|          | 186/581012 [01:43<89:55:21,  1.79it/s]Embedding watermark:   0%|          | 828/581012 [03:06<32:30:17,  4.96it/s]