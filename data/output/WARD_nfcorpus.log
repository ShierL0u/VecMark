nohup: ignoring input
/root/autodl-tmp/vectormark/utils1.py:17: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.

For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`
with: `from pydantic import BaseModel`
or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. 	from pydantic.v1 import BaseModel

  from KGW.KGW_claude import KGWWatermarkedLLM
============================================================
Processing nfcorpus dataset
============================================================
Processing dataset: nfcorpus
Using full dataset with 3633 samples
Final dataset size: 3633
Mapping file: data/mapping/WARD/nfcorpus_mapping_qwen3:8b.json
Output path: ./data/watermarked_data/WARD_nfcorpus.csv
wm_bits:  [1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1]
Loading model: ../model/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8

[32mINFO[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          
[32mINFO[0m   Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`          
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:01<00:08,  1.04s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:02<00:07,  1.06s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:03<00:06,  1.01s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:04<00:05,  1.01s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:05<00:04,  1.00s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:06<00:02,  1.00it/s]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:07<00:01,  1.01it/s]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:07<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 9/9 [00:08<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 9/9 [00:08<00:00,  1.02it/s]
[32mINFO[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.
[32mINFO[0m  Format: Converting GPTQ v1 to v2                                         
[32mINFO[0m  Format: Conversion complete: 0.018215417861938477s                       
[32mINFO[0m  Optimize: `TritonV2QuantLinear` compilation triggered.                   
Model loaded successfully
Loaded mapping data with 3633 entities
Column text has 3633 rows, max token length: 2300
output_llm_length: 2760 | output_token_length: 2760
Embedding watermark with facts:   0%|          | 0/3633 [00:00<?, ?it/s]